# ğŸ“˜ ScholarAI

ScholarAI is an AI-powered research paper assistant that lets users upload academic PDFs and interactively ask questions about their content. It uses vector search (ChromaDB) and a Retrieval-Augmented Generation (RAG) pipeline powered by LLaMA (Groq API) to generate grounded, citation-aware responses.

---

## ğŸš€ Features

- ğŸ“„ Upload research papers (PDF)
- ğŸ§  Contextual Q&A based on uploaded content
- ğŸ“š Handles multiple papers per session
- ğŸ“ Citation-aware answers
- ğŸš€ Fast inference using LLaMA via Groq

---

## ğŸ“¦ Tech Stack

| Component | Technology |
|-----------|------------|
| Backend | FastAPI |
| Vector Database | ChromaDB |
| Embeddings | BGE-small |
| LLM | LLaMA via Groq API |
| Parsing | pdfplumber & PyMuPDF |
| Deployment | Hugging Face Spaces |
| Frontend (planned) | Next.js |

---

## ğŸ“ Repository Structure

```
ScholarAI/
â”œâ”€â”€ backend/                # FastAPI backend code
â”‚   â”œâ”€â”€ app/
â”‚   â”‚   â”œâ”€â”€ routes/        # API endpoints
â”‚   â”‚   â”œâ”€â”€ ingest/        # PDF parsing & chunking
â”‚   â”‚   â”œâ”€â”€ rag/           # RAG logic
â”‚   â”‚   â””â”€â”€ session/       # Session manager
â”‚   â”œâ”€â”€ chroma_db/         # Vector storage
â”‚   â”œâ”€â”€ venv/              # Python environment (ignored)
â”‚   â””â”€â”€ requirements.txt
â”‚
â””â”€â”€ frontend/              # Next.js frontend (to be built)
```

---

## ğŸ”§ Installation (Backend)

Clone the repo and create a Python virtual environment:

```bash
git clone https://github.com/AnshAggr1303/ScholarAI
cd ScholarAI/backend
python3 -m venv venv
source venv/bin/activate   # macOS/Linux
venv\Scripts\activate      # Windows
pip install --upgrade pip setuptools wheel
pip install -r requirements.txt
```

---

## ğŸ“‘ Environment Variables

Create a `.env` file inside `backend/`:

```bash
GROQ_API_KEY=your_groq_api_key
```

---

## ğŸš€ Running Backend Locally

```bash
uvicorn app.main:app --reload --port 8000
```

---

## ğŸ“¡ API Endpoints

### ğŸ§ª Health Check
```
GET /health
```

### ğŸ“¤ Upload Paper
```
POST /papers/upload
```
**Form Data:**
- `file`: PDF file
- `session_id`: unique session ID

### ğŸ“„ List Active Papers
```
GET /papers?session_id={id}
```

### ğŸ’¬ Chat / Ask Question
```
POST /chat
```
**Body:**
```json
{
  "session_id": "abc123",
  "question": "Explain the methodology",
  "scope": "all"
}
```

---

## ğŸ“Œ Frontend (Next.js)

Frontend will be a separate Next.js project inside `frontend/`. It will communicate with the backend via REST API endpoints.

**Example service functions:**

```typescript
// services/api.ts
export async function uploadPaper(file, sessionId) { ... }
export async function sendMessage(sessionId, question) { ... }
```

---

## ğŸ§  How It Works (High Level)

1. **Upload PDF** â†’ extract text, chunk, embed, store in ChromaDB
2. **User Questions** â†’ retrieve most relevant chunks â†’ send to LLaMA via Groq â†’ return grounded answer
3. **Multi-Paper Sessions** â†’ maintain active papers in session â†’ filter retrieval accordingly

---

## ğŸ—‚ï¸ Recommended Workflow

1. Backend endpoints first
2. Frontend integration next
3. Deployment on Hugging Face Spaces

---

## ğŸ“„ Contribution

Feel free to open issues, add features, or improve prompts & UI. All contributions are welcome!

---

## ğŸ” License

MIT License Â© 2026

---

## ğŸ“¬ Contact

For questions or suggestions, open an issue or reach out via GitHub!